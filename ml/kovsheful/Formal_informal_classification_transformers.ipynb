{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e7831b1577d862c",
   "metadata": {},
   "source": [
    "# Formal vs informal classification for russian text using transofrmers architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493a272d9a48620c",
   "metadata": {},
   "source": [
    "## EDA and data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be3803f6cce1cf4",
   "metadata": {},
   "source": [
    "### Imports and params\n",
    "- `DOC_STRIDE = 128` creates a 25% overlap between 512-token windows during tokenization so the model can see transitions across chunks without exceeding the max length.\n",
    "- `RANDOM_SEED = 42` keeps train/val/test splits and shuffling deterministic so experiments are comparable.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9532124ff0baff82",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import textwrap\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from collections import Counter, defaultdict\n",
    "from datasets import Dataset, DatasetDict, Value\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 4)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "np.random.seed(42)\n",
    "\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6e59f20d4a490170",
   "metadata": {},
   "source": [
    "### Set data paths and verify they exist"
   ]
  },
  {
   "cell_type": "code",
   "id": "a5313507e0e02ccf",
   "metadata": {},
   "source": [
    "DATA_DIR = Path('..') / 'data'\n",
    "LENTA_PATH_SMALL = DATA_DIR / 'lenta_data.csv'\n",
    "LENTA_PATH = DATA_DIR / 'lenta_texts_2000-20.csv'\n",
    "ANY_PHRASES_PATH = DATA_DIR / 'dataset_any_phrases.csv'\n",
    "TAIGA_PATH = DATA_DIR / 'taiga_style_dataset_clean.csv'\n",
    "SOVA_PATH = DATA_DIR / 'sovadataset.csv'\n",
    "\n",
    "for path in (LENTA_PATH, LENTA_PATH_SMALL, ANY_PHRASES_PATH, TAIGA_PATH, SOVA_PATH):\n",
    "    if not path.exists():\n",
    "        print(f'Warning: missing file {path}')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "502e0b68",
   "metadata": {},
   "source": [
    "\n",
    "### `load_lenta_dataset`\n",
    "- **Logic**: Reads the newline-delimited Lenta dataset where every line is a formal news passage and converts it into the unified DataFrame schema for all data that would be used below (text, numeric label, human-readable label, source).\n",
    "- **Line by line**:\n",
    "  - `assert path.exists()`: fail fast if the expected file is missing\n",
    "  - Initialize `texts = []` and iterate through each raw line, stripping whitespace with `raw_line.strip()`\n",
    "  - `if text:` keeps only non-empty lines\n",
    "  - `assert texts`: guarantees at least one sample was read—if not, raising to avoid passing an empty frame downstream.\n",
    "  - `pd.DataFrame({'text': texts})`: wraps the collected strings into a DataFrame for further processing.\n",
    "  - `df['label'] = 1` / `df['label_name'] = 'formal'`: mark every record as formal since the corpus only contains official news.\n",
    "  - `df['source'] = 'lenta'`: keeps provenance so later analysis can filter by origin; the optional `source_prefix` argument lets us distinguish alternative Lenta datasets while leave ability to reuse this function\n",
    "  - `return df[...]`: select the standardized column order expected by later steps.\n",
    "- **Notes**: Keeping the loader minimal avoids extra transformations here; any deduplication or length filtering happens after all sources are merged so a single policy applies to everyone.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6b0268e233bf9d8d",
   "metadata": {},
   "source": [
    "def load_lenta_dataset(path: Path, source_prefix=\"\"):\n",
    "    assert path.exists(), f'Missing Lenta dataset: {path}'\n",
    "\n",
    "    texts = []\n",
    "    with path.open(encoding='utf-8') as handle:\n",
    "        for raw_line in handle:\n",
    "            text = raw_line.strip()\n",
    "            if text:\n",
    "                texts.append(text)\n",
    "\n",
    "    assert texts, f'No text rows found in {path}'\n",
    "\n",
    "    df = pd.DataFrame({'text': texts})\n",
    "    df['label'] = 1\n",
    "    df['label_name'] = 'formal'\n",
    "    df['source'] = 'lenta' + source_prefix\n",
    "    return df[['text', 'label', 'label_name', 'source']]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "13f41775",
   "metadata": {},
   "source": [
    "### `load_any_phrases_dataset`\n",
    "- **Logic**: Parses the informal messenger phrases dataset where each line starts with an optional numeric id followed by a comma and the quote, producing informal-labeled rows.\n",
    "- **Line by line**:\n",
    "  - Asserts file exist, iterate to strip whitespace and `continue` past blanks\n",
    "  - `parts = line.split(',', 1)`: split at most once; commas that belong to the phrase stay intact because we only separate the leading id portion.\n",
    "  - `text = parts[1] if len(parts) > 1 else parts[0]`: handles both `id,text` and `text`-only lines to keep the loader tolerant of minor format drift.\n",
    "  - `text.strip().strip('\"')`: remove surrounding whitespace and stray double quotes left in the raw dump.\n",
    "  - Append non-empty strings, verify list isn't empty\n",
    "  - `df['word_count'] = df['text'].str.split().map(len)` counts tokens by whitespace; `df = df[df['word_count'] >= min_words]` enforces the ≥5-word rule to drop extremely short acknowledgements because they are not suppored by design\n",
    "  - `df = df.drop(columns=['word_count'])` cleans up the helper column once filtering is done."
   ]
  },
  {
   "cell_type": "code",
   "id": "8230c0d7230f6b42",
   "metadata": {},
   "source": [
    "def load_any_phrases_dataset(path: Path, min_words: int = 5):\n",
    "    assert path.exists(), f'Missing dataset_any_phrases corpus: {path}'\n",
    "\n",
    "    texts = []\n",
    "    with path.open(encoding='utf-8') as handle:\n",
    "        for raw_line in handle:\n",
    "            line = raw_line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split(',', 1)\n",
    "            text = parts[1] if len(parts) > 1 else parts[0]\n",
    "            text = text.strip().strip('\"')\n",
    "            if text:\n",
    "                texts.append(text)\n",
    "\n",
    "    assert texts, f'No text rows found in {path}'\n",
    "\n",
    "    df = pd.DataFrame({'text': texts})\n",
    "    df['word_count'] = df['text'].str.split().map(len)\n",
    "    df = df[df['word_count'] >= min_words]\n",
    "    df = df.drop(columns=['word_count'])\n",
    "\n",
    "    df['label'] = 0\n",
    "    df['label_name'] = 'informal'\n",
    "    df['source'] = 'any_phrases'\n",
    "    return df[['text', 'label', 'label_name', 'source']]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4dc2fb4f",
   "metadata": {},
   "source": [
    "\n",
    "### `load_taiga_dataset`\n",
    "- **Logic**: Normalizes the Taiga style CSV, which already includes labels, by renaming columns, stripping whitespace, and mapping label ids to our two-class schema.\n",
    "- **Line by line**:\n",
    "  - Verify file is not missing, load the csv and rename text column to aligns the name with other datasets.\n",
    "  - `dropna(subset=['text', 'label'])` removes rows that lost either the text or its class during upstream preprocessing.\n",
    "  - `df['text'] = df['text'].astype(str).str.strip()` ensures even numeric-looking entries become strings and removes leading/trailing whitespaces\n",
    "  - Filter empty strings and convert labels to `int` to avoid potential issues in future\n",
    "  - `label_map = {1: 'formal', 0: 'informal'}` maps Taiga's label ids to our naming convention; `map` fills `label_name` accordingly.\n",
    "  - Set source and return DataFrame\n",
    "- **Notes**: We keep both classes because Taiga already distinguishes formality\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e502e93eb1e4bf1c",
   "metadata": {},
   "source": [
    "def load_taiga_dataset(path: Path):\n",
    "    assert path.exists(), f'Missing Taiga dataset: {path}'\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.rename(columns={'new_text': 'text'})\n",
    "    df = df.dropna(subset=['text', 'label'])\n",
    "    assert 'text' in df.columns, f'Missing text column: {path}'\n",
    "\n",
    "    df['text'] = df['text'].astype(str).str.strip()\n",
    "    df = df[df['text'] != '']\n",
    "    assert not df.empty, f'No text rows found in {path}'\n",
    "\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    label_map = {1: 'formal', 0: 'informal'}\n",
    "    df['label_name'] = df['label'].map(label_map)\n",
    "    df['source'] = 'taiga'\n",
    "    return df[['text', 'label', 'label_name', 'source']]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4cac2d13",
   "metadata": {},
   "source": [
    "\n",
    "### `load_sova_dataset`\n",
    "- **Logic**: Load csv, drops tiny snippets, and labels everything as informal since this corpus contains casual messenger replies only\n",
    "- **Line by line**:\n",
    "  - Verify file is not missing, load the csv and verify text column exisix\n",
    "  - `df = df[['text']].copy()` keeps only the text column—we ignore the provided `id`/`label` because id is not relevant and all text inside in informal\n",
    "  - Normalizes whitespace and guarantee string format, after that remove blank lines\n",
    "  - Logic with `df['word_count']` they same as for `load_any_phrases_dataset`\n",
    "  - Set `label=0`, `label_name='informal'`, `source='sova'`, and warn if nothing survives the filter so the user can adjust the threshold\n",
    "  - Return the standardized columns for concatenation with other datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5f8d90df75340920",
   "metadata": {},
   "source": [
    "def load_sova_dataset(path: Path, min_words: int = 5):\n",
    "    assert path.exists(), f'Missing Sova dataset: {path}'\n",
    "    df = pd.read_csv(path)\n",
    "    assert 'text' in df.columns, f'Missing text column: {path}'\n",
    "\n",
    "    df = df[['text']].copy()\n",
    "    df['text'] = df['text'].astype(str).str.strip()\n",
    "    df = df[df['text'] != '']\n",
    "\n",
    "    df['word_count'] = df['text'].str.split().map(len)\n",
    "    df = df[df['word_count'] >= min_words]\n",
    "    df = df.drop(columns=['word_count'])\n",
    "\n",
    "    df['label'] = 0\n",
    "    df['label_name'] = 'informal'\n",
    "    df['source'] = 'sova'\n",
    "    if df.empty:\n",
    "        print(f'Warning: no Sova rows with >= {min_words} words')\n",
    "    return df[['text', 'label', 'label_name', 'source']]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d33ea4ee",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset loading & sanity report\n",
    "Call every source-specific loader def, gather them into a dictionary, and print per-source counts so we immediately see label balance or missing files before merging\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "81b62a3e0e13bc46",
   "metadata": {},
   "source": [
    "lenta_df_small = load_lenta_dataset(LENTA_PATH_SMALL, \"_small\")\n",
    "lenta_df = load_lenta_dataset(LENTA_PATH)\n",
    "any_phrases_df = load_any_phrases_dataset(ANY_PHRASES_PATH)\n",
    "taiga_df = load_taiga_dataset(TAIGA_PATH)\n",
    "sova_df = load_sova_dataset(SOVA_PATH)\n",
    "\n",
    "datasets = {\n",
    "    'lenta_small': lenta_df_small,\n",
    "    'lenta': lenta_df,\n",
    "    'any_phrases': any_phrases_df,\n",
    "    'taiga': taiga_df,\n",
    "    'sova': sova_df,\n",
    "}\n",
    "for name, data in datasets.items():\n",
    "    print(f'{name} samples: {len(data):,}')\n",
    "    if not data.empty:\n",
    "        print(data['label_name'].value_counts())\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5fb4de1b",
   "metadata": {},
   "source": [
    "\n",
    "### Merge, deduplicate, and derive length stats\n",
    "- **Concept**: Combine all non-empty sources into a single DataFrame, drop exact text duplicates, and precompute character/word/token-length proxies that downstream EDA and bucketing rely on\n",
    "- **Details**:\n",
    "  - We raise if everything is empty to avoid silent failures\n",
    "  - `pd.concat(sources, ignore_index=True)` forms the unified dataset; `df.duplicated('text')` identifies identical transcripts, log how many rows are removed before resetting the index\n",
    "  - The length columns (`char_length`, `word_length`, `approx_token_length`) give multiple perspectives on transcript size. \n",
    "  - `TOKENS_TO_WORD_RATIO` approximates subword growth so we can reason about the 512-token window limit even before tokenizing. Its' value is approximated from the paper https://aclanthology.org/2021.acl-long.243.pdf Figure 1(b)\n",
    "- **Why**: Deduping prevents leakage between train/val/test splits (duplicate sentences would otherwise appear in multiple splits). Precomputing lengths lets us analyze coverage without running a tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "121dd207384d2e48",
   "metadata": {},
   "source": [
    "TOKENS_TO_WORD_RATIO = 1.3\n",
    "\n",
    "sources = []\n",
    "for name, data in datasets.items():\n",
    "    if data.empty:\n",
    "        print(f'Warning: dataset {name} is empty and will be skipped')\n",
    "    else:\n",
    "        sources.append(data)\n",
    "if not sources:\n",
    "    raise ValueError('No data loaded from any source.')\n",
    "\n",
    "df = pd.concat(sources, ignore_index=True)\n",
    "duplicates = df.duplicated(subset='text')\n",
    "print(f\"Dropping {duplicates.sum()} duplicate rows based on text column\")\n",
    "df = df[~duplicates].reset_index(drop=True)\n",
    "\n",
    "df['char_length'] = df['text'].str.len()\n",
    "df['word_length'] = df['text'].str.split().map(len)\n",
    "df['approx_token_length'] = (df['word_length'] * TOKENS_TO_WORD_RATIO).round().astype(int)\n",
    "display(df.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "984f9af1",
   "metadata": {},
   "source": [
    "### Label-level summary tables\n",
    "- Produce quick aggregate tables for counts, descriptive statistics, and length (>512 tokens) to understand class balance and sequence-length distribution before modeling.\n",
    "- **Details**:\n",
    "  - `length_summary = ...` aggregates mean/median/max across character, word, and approximate token lengths for each label\n",
    "  - `share_over_512 = ...` computes both absolute counts and percentages of samples whose approximate token length exceeds the 512-token BERT window, per label.\n",
    "- **Why**: These summaries validate that the merge worked, highlight any major imbalance"
   ]
  },
  {
   "cell_type": "code",
   "id": "6275b032edd1f682",
   "metadata": {},
   "source": [
    "label_counts = df.groupby('label_name').size().rename('samples').to_frame()\n",
    "length_summary = (\n",
    "    df.groupby('label_name')[['char_length', 'word_length', 'approx_token_length']]\n",
    "    .agg(['mean', 'median', 'max'])\n",
    "    .round(2)\n",
    ")\n",
    "share_over_512 = (\n",
    "    df.groupby('label_name')['approx_token_length']\n",
    "    .agg(\n",
    "        total='size',\n",
    "        over_512=lambda s: (s > 512).sum()\n",
    "    )\n",
    ")\n",
    "share_over_512['percent_over_512'] = (share_over_512['over_512'] / share_over_512['total']).round(4) * 100\n",
    "\n",
    "display(label_counts)\n",
    "display(length_summary)\n",
    "display(share_over_512)\n",
    "print(f'Total samples: {len(df):,}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6c45621f",
   "metadata": {},
   "source": [
    "Almost 5% of data is over ~512 tokens, which may require special handling during training, window sliding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c163600",
   "metadata": {},
   "source": [
    "### Draw statistics and show examples of data"
   ]
  },
  {
   "cell_type": "code",
   "id": "d7993c22e80d3124",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "for label_name, subset in df.groupby('label_name'):\n",
    "    axes[0].hist(subset['word_length'], bins=60, alpha=0.6, label=label_name)\n",
    "    axes[1].hist(subset['approx_token_length'], bins=60, alpha=0.6, label=label_name)\n",
    "axes[0].set_title('Word length distribution')\n",
    "axes[0].set_xlabel('Words per sample')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[1].set_title('Approximate token length distribution')\n",
    "axes[1].set_xlabel('Approx tokens per sample')\n",
    "axes[1].set_ylabel('Count')\n",
    "for ax in axes:\n",
    "    ax.legend()\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4d8581c1a67d456",
   "metadata": {},
   "source": [
    "def show_examples(source_df, label_name, n=3, width=120, seed=13):\n",
    "    subset = source_df[source_df['label_name'] == label_name]\n",
    "    if subset.empty:\n",
    "        print(f'No rows for label {label_name}')\n",
    "        return\n",
    "    sample = subset.sample(n=min(n, len(subset)), random_state=seed)\n",
    "    for idx, text in enumerate(sample['text'], start=1):\n",
    "        print(f'{label_name.upper()} #{idx}')\n",
    "        print(textwrap.fill(text, width=width))\n",
    "        print('-' * 80)\n",
    "\n",
    "\n",
    "show_examples(df, 'formal')\n",
    "show_examples(df, 'informal')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "39daa088",
   "metadata": {},
   "source": [
    "\n",
    "### Train/val/test split\n",
    "\n",
    "  - Using `indices` method with `loc` to split data by moving cursor over the data\n",
    "  - `TRAIN_FRAC = 0.8`, `VAL_FRAC = 0.1`, and `TEST_FRAC = 0.1` keeps most data for learning while reserving enough samples to tune hyperparameters and report final metrics; equal-sized val/test splits make comparisons fair.\n",
    "  - Shuffle with a fixed seed so every experiment sees the same partition"
   ]
  },
  {
   "cell_type": "code",
   "id": "6310509ce96208b9",
   "metadata": {},
   "source": [
    "TRAIN_FRAC = 0.8\n",
    "VAL_FRAC = 0.1\n",
    "TEST_FRAC = 0.1\n",
    "\n",
    "num_samples = len(df)\n",
    "indices = np.arange(num_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_end = int(TRAIN_FRAC * num_samples)\n",
    "train_idx = indices[:train_end]\n",
    "\n",
    "val_end = train_end + int(VAL_FRAC * num_samples)\n",
    "val_idx = indices[train_end:val_end]\n",
    "\n",
    "test_idx = indices[val_end:]\n",
    "\n",
    "df['split'] = 'test'\n",
    "df.loc[train_idx, 'split'] = 'train'\n",
    "df.loc[val_idx, 'split'] = 'val'\n",
    "print(f'Train samples: {len(train_idx):,}')\n",
    "print(f'Val samples: {len(val_idx):,}')\n",
    "print(f'Test samples: {len(test_idx):,}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d77869a7",
   "metadata": {},
   "source": [
    "### Split sanity table\n",
    "- **Why print this**: After slicing the dataset into train/val/test, we immediately inspect the cross-tab of split vs. label to ensure no class vanished in a split.\n",
    "- **Reading it**: Each row shows how many formal/informal samples ended up in each partition; ideally every split has both labels in roughly expected proportions."
   ]
  },
  {
   "cell_type": "code",
   "id": "eea2f952d1823f18",
   "metadata": {},
   "source": [
    "split_counts = (\n",
    "    df.groupby(['split', 'label_name'])\n",
    "    .size()\n",
    "    .rename('samples')\n",
    "    .to_frame()\n",
    "    .sort_index()\n",
    ")\n",
    "display(split_counts)\n",
    "\n",
    "for split, group in df.groupby('split'):\n",
    "    missing = {'formal', 'informal'} - set(group['label_name'])\n",
    "    if missing:\n",
    "        print(f'Warning: split {split} missing labels: {missing}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "71dbecaf",
   "metadata": {},
   "source": [
    "\n",
    "### Choosing the model, stride and random seed\n",
    "- **Model selection**: Used some of BERT models pretrained for Russian language understanding because of focus of language's features and sctucture.\n",
    "- `DOC_STRIDE = 128` - yields ~25 % overlap between 512-token windows, preserving context at chunk boundaries.\n",
    "- `RANDOM_SEED = 42` - keeps data splits and shuffling deterministic for consistent experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6be837f80d770a3",
   "metadata": {},
   "source": [
    "# Possible models list:\n",
    "## https://huggingface.co/cointegrated/rubert-tiny2, \"cointegrated/rubert-tiny2\"\n",
    "## https://huggingface.co/DeepPavlov/rubert-base-cased\n",
    "\n",
    "MODEL_NAME = \"DeepPavlov/rubert-base-cased\"\n",
    "MAX_SEQ_LENGTH = 512\n",
    "DOC_STRIDE = 128\n",
    "RANDOM_SEED = 42"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "17d237ea",
   "metadata": {},
   "source": [
    "\n",
    "### Preparing Hugging Face datasets & metadata\n",
    "- **Resetting indices**:After we deduplicate and merge everything, we call `df = df.reset_index(drop=True)` and then add `df['example_id'] = df.index`. That “example_id” is just a simple 0…N counter that uniquely tags every original message. Later, when tokenization creates sliding windows with `return_overflowing_tokens=True`, each window inherits the parent example_id\n",
    "- **Split snapshots**: `split_to_df` stores separate pandas frames for train/val/test, letting us inspect or export them independently before converting to `Dataset` objects\n",
    "- **`source_to_id` mapping**: Keeping a compact integer id per source helps with analysis (e.g., per-source metrics) without dragging string columns through the trainer; we attach it to every window later\n",
    "- **`DatasetDict` conversion**: Moving to Hugging Face `Dataset` objects now means tokenization, batching, and trainer hooks all operate on the same structure. Using `preserve_index=False` drops pandas' index to avoid carrying redundant columns\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "883cf619c3ccd1f5",
   "metadata": {},
   "source": [
    "df = df.reset_index(drop=True).copy()\n",
    "df['example_id'] = df.index\n",
    "split_to_df = {\n",
    "    split: df[df['split'] == split].reset_index(drop=True)\n",
    "    for split in ['train', 'val', 'test']\n",
    "}\n",
    "source_to_id = {name: idx for idx, name in enumerate(sorted(df['source'].unique()))}\n",
    "datasets_dict = DatasetDict({\n",
    "    split: Dataset.from_pandas(frame, preserve_index=False)\n",
    "    for split, frame in split_to_df.items()\n",
    "})\n",
    "source_to_id"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e1f6f41b",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15593d2a15fd49b0",
   "metadata": {},
   "source": [
    "#### **Tokenizer configuration**\n",
    "- **Checkpoint-aware tokenizer**: `AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)` loads the exact subword split rules that match the RuBERT checkpoint so ids align with the encoder. `use_fast=True` is enabled to rely on Hugging Face's Rust-backed tokenizer, which is significantly faster and exposes the `overflow_to_sample_mapping` metadata required for sliding windows\n",
    "\n",
    "#### **Sliding-window tokenization**\n",
    "- **Why sliding windows**: Approximately 5% of data could exceed BERT's 512-token limit. Instead of truncating, we split them into overlapping windows so the model sees every part of the message, and we can later aggregate logits per message\n",
    "- **Parameters explained**:\n",
    "  - `max_length=MAX_SEQ_LENGTH` caps each window at 512 tokens (BERT's limit)\n",
    "  - `padding=False` lets us keep variable-length windows; padding happens later in the collator so memory is only spent on the longest items in a batch\n",
    "  - `truncation=True` ensures the tokenizer doesn't error when a chunk is still longer than 512 (should never happen because we slide by `DOC_STRIDE`, but it's a safeguard)\n",
    "  - `stride=MAX_SEQ_LENGTH - DOC_STRIDE` creates overlap (e.g., 512-128=384 tokens carried over) so boundary sentences appear fully in at least one window\n",
    "  - `return_overflowing_tokens=True` emits extra rows for each additional window instead of truncating\n",
    "  - `return_attention_mask=True` keeps attention masks for later batching; special tokens mask is off because we don't need it\n",
    "- **Mapping and weights**:\n",
    "  - `overflow_to_sample_mapping` tells us which original example produced each window. We use it to copy labels, `example_id`, and `source_id` to every window\n",
    "  - `counts = Counter(mapping)` tracks how many windows each message spawned; we set `sample_weight = 1 / counts[idx]` so each message contributes the same total loss despite having multiple windows\n",
    "  - `source_id` uses the earlier `source_to_id` lookup so we can track per-source metrics later without string columns in the trainer\n",
    "- **Flow**: The function returns a flat dict of tokenized windows ready for Hugging Face `Dataset.map`, ensuring the trainer operates on consistent lists (`input_ids`, `attention_mask`, `labels`, `example_id`, `sample_weight`, `source_id`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9c604399f2a286cf",
   "metadata": {},
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "\n",
    "def tokenize_with_windows(examples):\n",
    "    \"\"\"\n",
    "    Tokenize a batch of transcripts using sliding windows so texts longer than\n",
    "    `MAX_SEQ_LENGTH` are split into overlapping 512-token segments.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : dict\n",
    "        Mini-batch pulled from the Hugging Face `Dataset` containing lists for\n",
    "        `text`, `label`, `example_id`, and `source`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Flat tokenized batch where every window has:\n",
    "        - `input_ids` / `attention_mask`\n",
    "        - `labels` copied from the parent example\n",
    "        - `example_id` so we can regroup windows later\n",
    "        - `sample_weight = 1 / num_windows_for_example` so each original message\n",
    "          contributes the same total loss\n",
    "        - `source_id` for per-source metrics\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - `return_overflowing_tokens=True` instructs the tokenizer to emit every\n",
    "      additional window instead of truncating after the first 512 tokens.\n",
    "    - `stride = MAX_SEQ_LENGTH - DOC_STRIDE` keeps 128 tokens of overlap so\n",
    "      sentences straddling the boundary appear intact in at least one window.\n",
    "    - `overflow_to_sample_mapping` reports which original example produced each\n",
    "      window (e.g., [0, 0, 0, 1, 1, ...]). We pop it from the tokenizer output,\n",
    "      convert it to a `Counter`, and use it to (a) copy metadata to each window\n",
    "      and (b) compute the per-window `sample_weight`.\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        stride=MAX_SEQ_LENGTH - DOC_STRIDE,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        return_special_tokens_mask=False,\n",
    "    )\n",
    "\n",
    "    mapping = tokenized.pop('overflow_to_sample_mapping')\n",
    "    counts = Counter(mapping)\n",
    "\n",
    "    tokenized['labels'] = [examples['label'][idx] for idx in mapping]\n",
    "    tokenized['example_id'] = [examples['example_id'][idx] for idx in mapping]\n",
    "    tokenized['sample_weight'] = [1.0 / counts[idx] for idx in mapping]\n",
    "    tokenized['source_id'] = [source_to_id[examples['source'][idx]] for idx in mapping]\n",
    "    return tokenized"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f6f635743587cd66",
   "metadata": {},
   "source": [
    "### Tokenized dataset cleanup\n",
    "- **Why drop columns**: once the text is tokenized there is no longer need the raw `text`, labels duplicated in pandas dtype, or other helper columns in the Hugging Face dataset—the trainer only needs tensors returned from `tokenize_with_windows`, so `remove_columns` keeps memory low and batches lean.\n",
    "- **`Dataset.map` usage**: applying `tokenize_with_windows` to each split via `.map(..., batched=True, desc='Tokenizing with sliding windows')` ensures the exact same logic (sliding windows, metadata copy, sample weights) runs across train/val/test.\n",
    "- **Real token length**: immediately after, a second `.map` calculates `length = len(input_ids)` for every window so later steps (length-aware batching, stats) use the true tokenizer length instead of the word-based approximation computed earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f6055b94770a2896",
   "metadata": {},
   "source": [
    "columns_to_remove = datasets_dict['train'].column_names\n",
    "tokenized_datasets = datasets_dict.map(\n",
    "    tokenize_with_windows,\n",
    "    batched=True,\n",
    "    remove_columns=columns_to_remove,\n",
    "    desc='Tokenizing with sliding windows'\n",
    ")\n",
    "tokenized_datasets = tokenized_datasets.map(\n",
    "    lambda batch: {'length': [len(ids) for ids in batch['input_ids']]},\n",
    "    batched=True,\n",
    "    desc='Computing segment lengths'\n",
    ")\n",
    "tokenized_datasets"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ebc8bbf1e4179e20",
   "metadata": {},
   "source": [
    "Display metrics of windows"
   ]
  },
  {
   "cell_type": "code",
   "id": "6379076a19525faf",
   "metadata": {},
   "source": [
    "for split, ds in tokenized_datasets.items():\n",
    "    window_counts = Counter(ds['example_id'])\n",
    "    avg_windows = sum(window_counts.values()) / len(window_counts)\n",
    "    max_windows = max(window_counts.values())\n",
    "    print(f\"Split: {split}\")\n",
    "    print(f\"  Total windows: {len(ds):,}\")\n",
    "    print(f\"  Avg windows per example: {avg_windows:.2f}\")\n",
    "    print(f\"  Max windows per example: {max_windows}\")\n",
    "    print(f\"  Class distribution: {Counter(ds['labels'])}\")\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5bf0b7fac82f2720",
   "metadata": {},
   "source": [
    "There is obvious shift towards formal in windows, but not critical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2eb3ec03795715",
   "metadata": {},
   "source": [
    "### Reproducibility and casting\n",
    "- `set_seed(RANDOM_SEED)` freezes Python, NumPy, and PyTorch Random number generations so every run with the same config produces identical splits, window order, and weight initializations\n",
    "- Enforces column types to avoid future problems"
   ]
  },
  {
   "cell_type": "code",
   "id": "29e006c6075cc55",
   "metadata": {},
   "source": [
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "for split in tokenized_datasets.keys():\n",
    "    tokenized_datasets[split] = tokenized_datasets[split].cast_column('sample_weight', Value('float32'))\n",
    "    tokenized_datasets[split] = tokenized_datasets[split].cast_column('labels', Value('int64'))\n",
    "    tokenized_datasets[split] = tokenized_datasets[split].cast_column('example_id', Value('int64'))\n",
    "    tokenized_datasets[split] = tokenized_datasets[split].cast_column('source_id', Value('int64'))\n",
    "    tokenized_datasets[split] = tokenized_datasets[split].cast_column('length', Value('int64'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7000379daef41619",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aaf669c5b7fe11",
   "metadata": {},
   "source": [
    "### Label dictionaries\n",
    "- Providing both `id2label` and `label2id` ensures the model's config names logits consistently for metrics and exporting, and logging `id2label` gives readable class names in reports."
   ]
  },
  {
   "cell_type": "code",
   "id": "1d8324176831ac16",
   "metadata": {},
   "source": [
    "id2label = {0: 'informal', 1: 'formal'}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "425cd24812d52503",
   "metadata": {},
   "source": [
    "### Freezing the encoder and clear classifier\n",
    "- Looping over `model.bert.parameters()` and setting `requires_grad=False` keeps the RuBERT encoder fixed; only the classification head update because of limited compute resources.\n",
    "- `model.classifier.reset_parameters()` wipes the classification head to a fresh random state so it doesn't inherit any biases from previous fine-tunes, giving the frozen encoder a clean readout layer."
   ]
  },
  {
   "cell_type": "code",
   "id": "11927799ccc5ea67",
   "metadata": {},
   "source": [
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier.reset_parameters()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "71f097842c92398a",
   "metadata": {},
   "source": [
    "### WeightedDataCollator logic\n",
    "- Collator role:  take the list of Python dictionaries that the dataset returns—each dict containing fields like input_ids, attention_mask, labels, + extra metadata we added and stack/pad them into PyTorch tensors so the model can consume the whole batch in one forward pass\n",
    "---\n",
    "- **Weighted's goal**: wrap Hugging Face's `DataCollatorWithPadding` so each batch keeps the per-window metadata we computed earlier (`sample_weight`, `example_id`, `source_id`). The base collator only handles padding `input_ids`/`attention_mask`; this wrapper pads first, then reattaches our custom tensors\n",
    "- **Line by line**:\n",
    "  - `sample_weight = torch.tensor(...)`: convert the Python floats to a tensor so they can participate in the loss computation. We keep them in the batch dict under `sample_weight` for the custom Trainer\n",
    "  - `example_id` / `source_id`: same idea—store them as tensors so we can track which original message (and which source dataset) each window belongs to. Useful for aggregation and diagnostics\n",
    "  - `for f in features: ... pop(...)`: remove the extra keys from each feature dict before calling the base collator. The base collator only expects standard tokenizer outputs; leaving our custom keys would make it crash.\n",
    "  - `batch = self.base_collator(features)`: pad `input_ids`/`attention_mask` to the longest sequence in the batch using the tokenizer's pad token, returning PyTorch tensors\n",
    "  - `batch['sample_weight'] = ...`: reattach the metadata tensors so the Trainer can consume them later\n",
    "- **Why this design**: keeping metadata out of the base collator avoids reimplementing padding logic, while still giving us a batch object that contains everything required for weighted loss and per-example bookkeeping\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f6d564feae6757dc",
   "metadata": {},
   "source": [
    "class WeightedDataCollator:\n",
    "    def __init__(self, base_collator):\n",
    "        self.base_collator = base_collator\n",
    "\n",
    "    def __call__(self, features):\n",
    "        sample_weight = torch.tensor([f['sample_weight'] for f in features], dtype=torch.float32)\n",
    "        example_id = torch.tensor([f['example_id'] for f in features], dtype=torch.int64)\n",
    "        source_id = torch.tensor([f['source_id'] for f in features], dtype=torch.int64)\n",
    "\n",
    "        for f in features:\n",
    "            f.pop('sample_weight')\n",
    "            f.pop('example_id')\n",
    "            f.pop('source_id')\n",
    "            f.pop('length', None)\n",
    "\n",
    "        batch = self.base_collator(features)\n",
    "        batch['sample_weight'] = sample_weight\n",
    "        batch['example_id'] = example_id\n",
    "        batch['source_id'] = source_id\n",
    "        return batch\n",
    "\n",
    "base_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest', return_tensors='pt')\n",
    "data_collator = WeightedDataCollator(base_collator)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6615f061",
   "metadata": {},
   "source": [
    "### TrainingArguments\n",
    "- **`output_dir='training/artifacts/rubert_formal_informal'`** – controls where checkpoints/metrics land to be able to get them from there\n",
    "- **`eval_strategy='epoch'`** – tells Trainer to run validation after each epoch. Because our dataset is mid-sized and we care about message-level F1, per-epoch eval is frequent enough to catch regressions without slowing training too much\n",
    "- **`save_strategy='epoch'`** – mirrors the eval cadence so every validation pass has a matching checkpoint. This is required for early stopping / best-model logic to align with eval results\n",
    "- **`logging_strategy='epoch'`** – aggregate logs once per epoch to avoid noisy per-step logs\n",
    "- **`per_device_train_batch_size=16` / `per_device_eval_batch_size=16`** – batch size per accelerator. On MPS this fits comfortably once the encoder is frozen; it keeps gradient noise low while leaving headroom for sliding windows\n",
    "- **`gradient_accumulation_steps=2`** – effectively doubles the batch to 32 examples without needing more VRAM. We accumulate two mini-batches before stepping the optimizer so the frozen encoder still sees a stable gradient\n",
    "- **`learning_rate=2e-5`** – standard fine-tuning LR for BERT-style encoders. Even though we’re training mostly the head, sticking to 2e-5 keeps optimizer behavior familiar and avoids overshooting\n",
    "- **`num_train_epochs=3`** – typical sweet spot for RuBERT; gives the head enough passes to converge while keeping experiments short (~<6h target). We rely on early stopping if validation stops improving sooner\n",
    "- **`weight_decay=0.01`** – light L2 regularization to prevent the classifier head from overfitting the training windows. 0.01 is the Transformers default tuned for AdamW\n",
    "- **`warmup_ratio=0.06`** – gradually ramps LR over the first 6% of training steps to avoid sudden jumps. Helpful when gradients start from scratch due to the reset head\n",
    "- **`logging_steps=1`** – since we log per-epoch overall, this only affects the internal Trainer logs\n",
    "- **`save_total_limit=3`** – keep only the latest/best three checkpoints so disk usage stays low while still letting us roll back if needed\n",
    "- **`load_best_model_at_end=True`** – after training, automatically reloads the checkpoint with the best validation metric; saves us from manually tracking which epoch won\n",
    "- **`metric_for_best_model='f1_macro'`** / **`greater_is_better=True`** – defines the ranking metric so the best checkpoint is chosen by macro-F1 (balanced between classes), maximizing the score we actually report\n",
    "- **`fp16=False`** – disable mixed precision; stick to float32 for predictable training\n",
    "- **`group_by_length=True`** / **`length_column_name='length'`** – batches windows of similar token length to reduce padding waste; uses the true tokenizer length we computed earlier\n",
    "- **`remove_unused_columns=False`** – keeps `sample_weight`, `example_id`, and `source_id` in the datasets so our custom trainer can access them\n",
    "- **`seed=RANDOM_SEED`** – duplicates the global randomness lock inside `TrainingArguments` so Trainer-controlled RNG streams stay deterministic\n",
    "- **`use_mps_device=True`** / **`dataloader_pin_memory=False`** – tells Trainer to prefer Apple’s MPS backend (faster than CPU) and disables pinning since PyTorch warns it isn’t supported on MPS yet\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a845c62968cdd504",
   "metadata": {},
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='training/artifacts/rubert_formal_informal',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.06,\n",
    "    logging_steps=1,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_macro',\n",
    "    greater_is_better=True,\n",
    "    fp16=False,\n",
    "    group_by_length=True,\n",
    "    length_column_name='length',\n",
    "    remove_unused_columns=False,\n",
    "    seed=RANDOM_SEED,\n",
    "    use_mps_device=True,\n",
    "    dataloader_pin_memory=False,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "81f38480",
   "metadata": {},
   "source": [
    "### Trainer & metrics\n",
    "- **Why a custom trainer**: sliding windows mean one original message can produce 3–5 training rows. Without weighting, those rows would contribute 3–5× the gradient of a short message. Subclassing `Trainer` and overriding `compute_loss` lets the model consume the `sample_weight` tensor so each original message still counts as 1\n",
    "- **Metadata handling**: `inputs` initially contains `input_ids`, `attention_mask`, `labels`, plus metadata (`sample_weight`, `example_id`, `source_id`, `length`). The overridden `compute_loss` pops the metadata keys before calling the base model (it would error on unknown kwargs), but holds onto them for loss weighting or later analysis\n",
    "- **Loss computation (example)**: suppose `example_id 7` was split into 3 windows, so each window’s `sample_weight` is `1/3 ≈ 0.33`. Cross-entropy is computed per window (`reduction='none'`), then multiplied by 0.33 before averaging the batch. The sum of the three weighted losses equals what a single window would have contributed, keeping the gradient per message constant\n",
    "- **`compute_metrics`**: converts logits to predictions and reports accuracy + macro/weighted F1. Macro-F1 is optimized because it forces balanced performance on formal and informal texts even when the dataset skews\n",
    "- **Trainer wiring**: `WeightedLossTrainer(...)` ties together the model, arguments, datasets, tokenizer, collator, and metrics so the weighted loss/metadata flow happens automatically each step. An early-stopping callback monitors validation macro-F1 and halts if it stalls for two epochs, keeping training efficient"
   ]
  },
  {
   "cell_type": "code",
   "id": "dce1cb7ce274056b",
   "metadata": {},
   "source": [
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None, **kwargs):\n",
    "        sample_weight = inputs.pop('sample_weight')\n",
    "        inputs.pop('example_id', None)\n",
    "        inputs.pop('source_id', None)\n",
    "        inputs.pop('length', None)\n",
    "        labels = inputs.pop('labels')\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        losses = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        weighted_loss = (losses * sample_weight.to(losses.device)).mean()\n",
    "\n",
    "        if return_outputs:\n",
    "            return weighted_loss, outputs\n",
    "        return weighted_loss\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1_macro': f1_score(labels, preds, average='macro'),\n",
    "        'f1_weighted': f1_score(labels, preds, average='weighted'),\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d6606d2c30b801e3",
   "metadata": {},
   "source": [
    "- **`WeightedLossTrainer(...)` arguments**:\n",
    "  - `model`: the frozen RuBERT encoder plus reinitialized classifier head\n",
    "  - `args`: the `TrainingArguments` block that governs batching, logging, saving; passing it in ensures the Trainer obeys those settings\n",
    "  - `train_dataset` / `eval_dataset`: the tokenized Hugging Face splits, allowing the Trainer to iterate and evaluate over it\n",
    "  - `processing_class=tokenizer`: bundles the tokenizer with checkpoints so reloading the model later recreates the same subword vocab\n",
    "  - `data_collator`: supplies the padded batches with `sample_weight`/`example_id` tensors that the custom loss expects.\n",
    "  - `compute_metrics`: hooks up the accuracy/F1 reporter for evaluation\n",
    "- **Early stopping**: `trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=2))` stops training if validation macro-F1 fails to improve for two evaluation cycles, keeping runtime bounded and preventing the head from overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "4f4e10b39ae5f758",
   "metadata": {},
   "source": [
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['val'],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=2))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d8b88c3eed0a78c8",
   "metadata": {},
   "source": [
    "# test_metrics = trainer.evaluate(tokenized_datasets['test'])\n",
    "# trainer.log_metrics('test', test_metrics)\n",
    "# trainer.save_metrics('test', test_metrics)\n",
    "# test_metrics\n",
    "\n",
    "# val_preds = trainer.predict(tokenized_datasets['val'])\n",
    "# test_preds = trainer.predict(tokenized_datasets['test'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "258002d1",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "- `train_result = trainer.train()` launches fine-tuning with all previously defined wiring (sliding windows, weighted loss, early stopping)\n",
    "- Saving the model, logging metrics, and persisting the Trainer state immediately afterwards captures everything needed to resume or review the run later\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "56709298b942f6d7",
   "metadata": {},
   "source": [
    "train_result = trainer.train()\n",
    "trainer.save_model()\n",
    "train_metrics = train_result.metrics\n",
    "trainer.log_metrics('train', train_metrics)\n",
    "trainer.save_metrics('train', train_metrics)\n",
    "trainer.save_state()\n",
    "train_metrics"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f445266a",
   "metadata": {},
   "source": [
    "### Validation & test evaluation\n",
    "- `trainer.evaluate(tokenized_datasets['val'])` runs the validation split using the best checkpoint (because `load_best_model_at_end=True`), logging and saving the raw validation metrics over windows\n",
    "- After that call `trainer.predict(tokenized_datasets['val'])` does the same on the test split, returning raw logits and labels for window-level predictions. We need them o regroup sliding windows into message-level predictions\n",
    "- The following code repeats the process on the test split"
   ]
  },
  {
   "cell_type": "code",
   "id": "4b4fab15b0fdf6e2",
   "metadata": {},
   "source": [
    "val_metrics = trainer.evaluate(tokenized_datasets['val'])\n",
    "trainer.log_metrics('eval', val_metrics)\n",
    "trainer.save_metrics('eval', val_metrics)\n",
    "val_metrics\n",
    "val_preds = trainer.predict(tokenized_datasets['val'])\n",
    "\n",
    "test_metrics = trainer.evaluate(tokenized_datasets['test'])\n",
    "trainer.log_metrics('test', test_metrics)\n",
    "trainer.save_metrics('test', test_metrics)\n",
    "test_metrics\n",
    "\n",
    "test_preds = trainer.predict(tokenized_datasets['test'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6ca0aa75",
   "metadata": {},
   "source": [
    "### Message-level aggregation & reporting\n",
    "- **Goal**: convert per-window predictions back into per-message metrics so evaluation reflects the original transcripts, not individual sliding windows\n",
    "- **`aggregate_message_logits`**\n",
    "  - Inputs: raw window logits (`logits`), true labels (`labels`), and `example_ids` identifying the parent message for each window\n",
    "  - It buckets windows by `example_id`, stacks their logits, and pools them (`mean` by default, `max` optional) to obtain **one logit vector per message**. The function returns three arrays: pooled logits, the true label per message, and the message ids\n",
    "  - Example: if message produced three windows, the bucket contains three logits. `np.vstack` stacks them, `mean` averages across rows → resulting pooled logit is used to decide the single prediction for this message\n",
    "- **`report_message_metrics`**\n",
    "  - Calls `aggregate_message_logits`, converts logits to predictions via `argmax`, and prints a `classification_report` plus confusion matrix labeled with `id2label`. This gives accuracy/F1 **per original message**\n",
    "  - Returns a dictionary containing the per-message logits, labels, predicted labels, example ids, and confusion matrix—handy for further analysis or saving to disk\n",
    "- **Usage**: runs `report_message_metrics` on both validation and test splits immediately after `trainer.predict`. The window-level metrics from `trainer.evaluate` are quick checks; these aggregated numbers are the authoritative scores for model selection and reporting\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "dd8a74796eb4214",
   "metadata": {},
   "source": [
    "def aggregate_message_logits(logits, labels, example_ids, reduction='mean'):\n",
    "    buckets = defaultdict(list)\n",
    "    label_map = {}\n",
    "    for logit, label, eid in zip(logits, labels, example_ids):\n",
    "        buckets[int(eid)].append(logit)\n",
    "        label_map[int(eid)] = int(label)\n",
    "    agg_logits = []\n",
    "    agg_labels = []\n",
    "    agg_ids = []\n",
    "    for eid, chunk in buckets.items():\n",
    "        stack = np.vstack(chunk)\n",
    "        if reduction == 'max':\n",
    "            pooled = stack.max(axis=0)\n",
    "        else:\n",
    "            pooled = stack.mean(axis=0)\n",
    "        agg_logits.append(pooled)\n",
    "        agg_labels.append(label_map[eid])\n",
    "        agg_ids.append(eid)\n",
    "    return np.vstack(agg_logits), np.array(agg_labels), np.array(agg_ids)\n",
    "\n",
    "\n",
    "def report_message_metrics(split_name, dataset, preds_output):\n",
    "    logits, labels, example_ids = (\n",
    "        preds_output.predictions,\n",
    "        preds_output.label_ids,\n",
    "        dataset['example_id'],\n",
    "    )\n",
    "    msg_logits, msg_labels, msg_ids = aggregate_message_logits(logits, labels, example_ids)\n",
    "    msg_pred_labels = msg_logits.argmax(axis=-1)\n",
    "    print(f\"Message-level metrics for {split_name} split\")\n",
    "    print(classification_report(msg_labels, msg_pred_labels, target_names=[id2label[0], id2label[1]]))\n",
    "    cm = confusion_matrix(msg_labels, msg_pred_labels)\n",
    "    print(cm)\n",
    "    return {\n",
    "        'logits': msg_logits,\n",
    "        'labels': msg_labels,\n",
    "        'pred_labels': msg_pred_labels,\n",
    "        'example_ids': msg_ids,\n",
    "        'confusion_matrix': cm,\n",
    "    }\n",
    "    \n",
    "\n",
    "val_message_stats = report_message_metrics('val', tokenized_datasets['val'], val_preds)\n",
    "test_message_stats = report_message_metrics('test', tokenized_datasets['test'], test_preds)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bfd90492",
   "metadata": {},
   "source": [
    "### Summarizing test metrics\n",
    "- Uses `classification_report` on the message-level predictions to produce precision/recall/F1 per class, then converts it to a tidy DataFrame for inspection or logging. This is the aggregate score to record for the test split\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5319b3fede348627",
   "metadata": {},
   "source": [
    "test_report = classification_report(\n",
    "    test_message_stats['labels'],\n",
    "    test_message_stats['pred_labels'],\n",
    "    target_names=[id2label[0], id2label[1]],\n",
    "    output_dict=True,\n",
    ")\n",
    "pd.DataFrame(test_report).T"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6127ae2a",
   "metadata": {},
   "source": [
    "### Inspecting per-message predictions\n",
    "- Builds a DataFrame from `test_message_stats` so each row corresponds to an original transcript with its true label, predicted label, and aggregated class probabilities\n",
    "- Displaying `.head()` provides a quick sanity check on the aggregated outputs"
   ]
  },
  {
   "cell_type": "code",
   "id": "79fd5adaeccdc441",
   "metadata": {},
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'example_id': test_message_stats['example_ids'],\n",
    "    'true_label': [id2label[i] for i in test_message_stats['labels']],\n",
    "    'pred_label': [id2label[i] for i in test_message_stats['pred_labels']],\n",
    "    'P(informal)': torch.softmax(torch.tensor(test_message_stats['logits']), dim=-1).numpy()[:, 0],\n",
    "    'P(formal)': torch.softmax(torch.tensor(test_message_stats['logits']), dim=-1).numpy()[:, 1],\n",
    "})\n",
    "results_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
